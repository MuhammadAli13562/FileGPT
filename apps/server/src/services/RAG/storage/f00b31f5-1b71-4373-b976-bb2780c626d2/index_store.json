{"docstore/data":{"9e01be3e-fa25-4c4e-82f5-4e08957266d0":{"indexId":"9e01be3e-fa25-4c4e-82f5-4e08957266d0","nodesDict":{"3b4425ee-4fca-4c39-b5d0-b1d656028b7f":{"id_":"3b4425ee-4fca-4c39-b5d0-b1d656028b7f","metadata":{},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"dd925f8f-3212-4285-920d-2735a7aee0a1","metadata":{},"hash":"UQMlcBaSuNqkLsQgh0M2eMpHFVdYg/bbm4KWQ0MiP8M="},"NEXT":{"nodeId":"3605e60b-ec28-4d5d-b859-e8b5d210c1ba","metadata":{},"hash":"pYVbiz2h6tFpBlnkJmWLx5iRPjssEa2XBSZkcqLL/wY="}},"hash":"oS7zruNc2vr4RcupBpmFbmIJt4V9c2ZjnXTCbM8GqP8=","text":"CS 229 – Machine Learninghttps://stanford.edu/~shervine\nVIP Cheatsheet:  Machine Learning Tips\nAfshineAmidiand ShervineAmidi\nSeptember 9, 2018\nMetrics\nGiven a set of data points{x\n(1)\n,...,x\n(m)\n}, where eachx\n(i)\nhasnfeatures, associated to a set of\noutcomes{y\n(1)\n,...,y\n(m)\n}, we want to assess a given classifier that learns how to predictyfrom\nx. Classification\nIn a context of a binary classification, here are the main metrics that are important to track to\nassess the performance of the model. rConfusion matrix– The confusion matrix is used to have a more complete picture when\nassessing the performance of a model. It is defined as follows:\nPredictedclass\n+–\nActualclass\nTPFN\n+False Negatives\nTrue Positives\nType II error\nFPTN\n–False Positives\nTrue Negatives\nType I error\nrMain metrics– The following metrics are commonly used to assess the performance of\nclassification models:\nMetricFormulaInterpretation\nAccuracy\nTP+TN\nTP+TN+FP+FN\nOverall performance of model\nPrecision\nTP\nTP+FP\nHow accurate the positive predictions are\nRecall\nTP\nTP+FN\nCoverage of actual positive sample\nSensitivity\nSpecificity\nTN\nTN+FP\nCoverage of actual negative sample\nF1 score\n2TP\n2TP+FP+FN\nHybrid metric useful for unbalanced classes\nrROC– The receiver operating curve, also noted ROC, is the plot of TPR versus FPR by\nvarying the threshold. These metrics are are summed up in the table below:\nMetricFormulaEquivalent\nTrue Positive Rate\nTP\nTP+FN\nRecall, sensitivity\nTPR\nFalse Positive Rate\nFP\nTN+FP\n1-specificity\nFPR\nrAUC– The area under the receiving operating curve, also noted AUC or AUROC, is the\narea below the ROC as shown in the following figure:\nRegression\nrBasic metrics– Given a regression modelf, the following metrics are commonly used to\nassess the performance of the model:\nTotal sum of squaresExplained sum of squaresResidual sum of squares\nSS\ntot\n=\nm\n∑\ni=1\n(y\ni\n−y)\n2\nSS\nreg\n=\nm\n∑\ni=1\n(f(x\ni\n)−y)\n2\nSS\nres\n=\nm\n∑\ni=1\n(y\ni\n−f(x\ni\n))\n2\nrCoefficient of determination– The coefficient of determination, often notedR\n2\norr\n2\n,\nprovides a measure of how well the observed outcomes are replicated by the model and is defined\nas follows:\nR\n2\n= 1−\nSS\nres\nSS\ntot\nrMain metrics– The following metrics are commonly used to assess the performance of\nregression models, by taking into account the number of variablesnthat they take into consid-\neration:\nMallow’s CpAICBICAdjustedR\n2\nSS\nres\n+ 2(n+ 1)̂σ\n2\nm\n2\n[\n(n+ 2)−log(L)\n]\nlog(m)(n+ 2)−2 log(L)1−\n(1−R\n2\n)(m−1)\nm−n−1\nStanford University1Fall 2018\n\nCS 229 – Machine Learninghttps://stanford.edu/~shervine\nwhereLis the likelihood and̂σ\n2\nis an estimate of the variance associated with each response. Model selection\nrVocabulary– When selecting a model, we distinguish 3 different parts of the data that we\nhave as follows:\nTraining setValidation setTesting set\n- Model is trained- Model is assessed- Model gives predictions\n- Usually 80% of the dataset- Usually 20% of the dataset- Unseen data\n- Also called hold-out\nor development set\nOnce the model has been chosen, it is trained on the entire dataset and tested on the unseen\ntest set. These are represented in the figure below:\nrCross-validation– Cross-validation, also noted CV, is a method that is used to select a\nmodel that does not rely too much on the initial training set.","textTemplate":"","metadataSeparator":"\n","type":"TEXT"},"3605e60b-ec28-4d5d-b859-e8b5d210c1ba":{"id_":"3605e60b-ec28-4d5d-b859-e8b5d210c1ba","metadata":{},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"dd925f8f-3212-4285-920d-2735a7aee0a1","metadata":{},"hash":"UQMlcBaSuNqkLsQgh0M2eMpHFVdYg/bbm4KWQ0MiP8M="},"PREVIOUS":{"nodeId":"3b4425ee-4fca-4c39-b5d0-b1d656028b7f","metadata":{},"hash":"oS7zruNc2vr4RcupBpmFbmIJt4V9c2ZjnXTCbM8GqP8="}},"hash":"pYVbiz2h6tFpBlnkJmWLx5iRPjssEa2XBSZkcqLL/wY=","text":"The different types are summed\nup in the table below:\nk-foldLeave-p-out\n- Training onk−1folds and- Training onn−pobservations and\nassessment on the remaining oneassessment on thepremaining ones\n- Generallyk= 5or10- Casep= 1is called leave-one-out\nThe most commonly used method is calledk-fold cross-validation and splits the training data\nintokfolds to validate the model on one fold while training the model on thek−1other folds,\nall of thisktimes. The error is then averaged over thekfolds and is named cross-validation\nerror. rRegularization– The regularization procedure aims at avoiding the model to overfit the\ndata and thus deals with high variance issues. The following table sums up the different types\nof commonly used regularization techniques:\nLASSORidgeElastic Net\n- Shrinks coefficients to 0Makes coefficients smallerTradeoff between variable\n- Good for variable selectionselection and small coefficients\n...+λ||θ||\n1\n...+λ||θ||\n2\n2\n...+λ\n[\n(1−α)||θ||\n1\n+α||θ||\n2\n2\n]\nλ∈Rλ∈Rλ∈R,  α∈[0,1]\nrModel selection– Train model on training set, then evaluate on the development set, then\npick best performance model on the development set, and retrain all of that model on the whole\ntraining set. Diagnostics\nrBias– The bias of a model is the difference between the expected prediction and the correct\nmodel that we try to predict for given data points. rVariance– The variance of a model is the variability of the model prediction for given data\npoints. rBias/variance tradeoff– The simpler the model, the higher the bias, and the more complex\nthe model, the higher the variance. UnderfittingJust rightOverfitting\n- High training error- Training error- Low training error\nSymptoms- Training error closeslightly lower than- Training error much\nto test errortest errorlower than test error\n- High bias- High variance\nRegression\nStanford University2Fall 2018\n\nCS 229 – Machine Learninghttps://stanford.edu/~shervine\nClassification\nDeep learning\n- Complexify model- Regularize\nRemedies- Add more features- Get more data\n- Train longer\nrError analysis– Error analysis is analyzing the root cause of the difference in performance\nbetween the current and the perfect models. rAblative analysis– Ablative analysis is analyzing the root cause of the difference in perfor-\nmance between the current and the baseline models. Stanford University3Fall 2018","textTemplate":"","metadataSeparator":"\n","type":"TEXT"}},"type":"simple_dict"}}}